{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga de datasets\n",
    "from utils.DatasetStorage import Dataset\n",
    "from utils.paths import *\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.datasets import mnist\n",
    "from keras import optimizers\n",
    "\n",
    "# clasificadores\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#variables para los datasets\n",
    "data_names = ['amazon', 'twitter', 'twitter_3']\n",
    "\n",
    "#variables para guardar los resultados\n",
    "tipo = 'sda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print x_train.shape\n",
    "print x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784, ))\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = GaussianNoise(1)(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs=1,\n",
    "               batch_size=256,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_test,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed = encoder.predict(x_test[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desde aqui importa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_SDA(input_size, layers):\n",
    "    input_layer = Input(shape=(input_size,))    \n",
    "    encoded = None\n",
    "    decoded = None\n",
    "    \n",
    "    for layer in layers:\n",
    "        if encoded is None:\n",
    "            encoded = Dense(layer, activation='sigmoid')(input_layer)\n",
    "            #masking noise\n",
    "            encoded = GaussianNoise(1)(encoded)\n",
    "        else:\n",
    "            encoded = Dense(layer, activation='softplus')(encoded)\n",
    "            \n",
    "    \n",
    "    for layer in layers[-2::-1]:\n",
    "        if decoded is None:\n",
    "            decoded = Dense(layer, activation='relu')(encoded)\n",
    "        else:\n",
    "            decoded = Dense(layer, activation='relu')(decoded)\n",
    "       \n",
    "    if len(layers) == 1:\n",
    "            decoded = Dense(input_size, activation='sigmoid')(encoded)\n",
    "    else:\n",
    "        decoded = Dense(input_size, activation='sigmoid')(decoded)\n",
    "    \n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "    \n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dataset de Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already splitted\n"
     ]
    }
   ],
   "source": [
    "# cargando dataset Amazon\n",
    "dataset_path = save_path+data_names[0]+'.pkl'\n",
    "dataset_object = Dataset().load(dataset_path)\n",
    "\n",
    "dataset_object.split_dataset(test_size=0.2)\n",
    "\n",
    "labeled = dataset_object.labeled\n",
    "unlabeled = dataset_object.unlabeled\n",
    "domains = dataset_object.domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los datos disponibles obtenidos\n"
     ]
    }
   ],
   "source": [
    "# se obtienen todos los valores X disponibles para realizar adaptacion\n",
    "X = dataset_object.get_all_X()\n",
    "X = np.asarray(X)\n",
    "\n",
    "print \"Todos los datos disponibles obtenidos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dims = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#autoencoder, encoder = create_SDA(784, [128, 64, 32, 16, 8])\n",
    "autoencoder, encoder = create_SDA(dims, [500, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27677 samples, validate on 27677 samples\n",
      "Epoch 1/15\n",
      "27677/27677 [==============================] - 13s - loss: 0.1163 - val_loss: 0.1190\n",
      "Epoch 2/15\n",
      "27677/27677 [==============================] - 13s - loss: 0.1064 - val_loss: 0.1092\n",
      "Epoch 3/15\n",
      "27677/27677 [==============================] - 13s - loss: 0.0998 - val_loss: 0.1026\n",
      "Epoch 4/15\n",
      "27677/27677 [==============================] - 13s - loss: 0.0955 - val_loss: 0.0980\n",
      "Epoch 5/15\n",
      "27677/27677 [==============================] - 14s - loss: 0.0924 - val_loss: 0.0947\n",
      "Epoch 6/15\n",
      "27677/27677 [==============================] - 15s - loss: 0.0903 - val_loss: 0.0922\n",
      "Epoch 7/15\n",
      "27677/27677 [==============================] - 14s - loss: 0.0888 - val_loss: 0.0903\n",
      "Epoch 8/15\n",
      "27677/27677 [==============================] - 15s - loss: 0.0876 - val_loss: 0.0889\n",
      "Epoch 9/15\n",
      "27677/27677 [==============================] - 15s - loss: 0.0867 - val_loss: 0.0878\n",
      "Epoch 10/15\n",
      "27677/27677 [==============================] - 16s - loss: 0.0861 - val_loss: 0.0869\n",
      "Epoch 11/15\n",
      "27677/27677 [==============================] - 16s - loss: 0.0856 - val_loss: 0.0862\n",
      "Epoch 12/15\n",
      "27677/27677 [==============================] - 15s - loss: 0.0852 - val_loss: 0.0857\n",
      "Epoch 13/15\n",
      "27677/27677 [==============================] - 15s - loss: 0.0848 - val_loss: 0.0852\n",
      "Epoch 14/15\n",
      "27677/27677 [==============================] - 16s - loss: 0.0845 - val_loss: 0.0848\n",
      "Epoch 15/15\n",
      "27677/27677 [==============================] - 15s - loss: 0.0843 - val_loss: 0.0845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f583c270090>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:, :dims]\n",
    "\n",
    "autoencoder.fit(X_train, X_train,\n",
    "               epochs=15,\n",
    "               batch_size=256,\n",
    "               shuffle=True,\n",
    "               validation_data=(X_train, X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptando kitchen\n",
      "Adaptando dvd\n",
      "Adaptando electronics\n",
      "Adaptando books\n"
     ]
    }
   ],
   "source": [
    "#diccionario para mantener los dominios adaptados\n",
    "adapted = {}\n",
    "\n",
    "for domain in domains:\n",
    "    print \"Adaptando %s\" % domain\n",
    "    X_tr = np.asarray(labeled[domain]['X_tr'][:, :dims].todense())\n",
    "    X_ts = np.asarray(labeled[domain]['X_ts'][:, :dims].todense())\n",
    "    \n",
    "    tr_reps = encoder.predict(X_tr[:, :dims])\n",
    "    ts_reps = encoder.predict(X_ts[:, :dims])\n",
    "    \n",
    "    adapted[domain] = {\n",
    "        'X_tr': tr_reps,\n",
    "        'X_ts': ts_reps\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea 1 de 12\n",
      "0.28  - 0.368\n",
      "Tarea 2 de 12\n",
      "0.2875  - 0.360\n",
      "Tarea 3 de 12\n",
      "0.3225  - 0.420\n",
      "Tarea 4 de 12\n",
      "0.24  - 0.397\n",
      "Tarea 5 de 12\n",
      "0.2875  - 0.410\n",
      "Tarea 6 de 12\n",
      "0.3225  - 0.360\n",
      "Tarea 7 de 12\n",
      "0.24  - 0.380\n",
      "Tarea 8 de 12\n",
      "0.28  - 0.365\n",
      "Tarea 9 de 12\n",
      "0.3225  - 0.432\n",
      "Tarea 10 de 12\n",
      "0.24  - 0.377\n",
      "Tarea 11 de 12\n",
      "0.28  - 0.310\n",
      "Tarea 12 de 12\n",
      "0.2875  - 0.385\n",
      "Pruebas completadas.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Adaptacion','Dataset','Tarea','Fuente', 'Objetivo','Baseline error', 'Transfer error', 'Transfer loss'])\n",
    "\n",
    "\n",
    "i = 0\n",
    "for src in domains:\n",
    "    for tgt in domains:\n",
    "        if src is not tgt:\n",
    "            print \"Tarea %d de 12\" % (i+1)\n",
    "            #baseline in-domain error\n",
    "            #e_b(T,T)\n",
    "            #entrenado en dominio tgt y probado en dominio tgt\n",
    "            X_tr = np.asarray(labeled[tgt]['X_tr'][:, :dims].todense())\n",
    "            y_tr = np.asarray(labeled[tgt]['y_tr'].todense()).argmax(axis=1)\n",
    "            \n",
    "            X_ts = np.asarray(labeled[tgt]['X_ts'][:, :dims].todense())\n",
    "            y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "            \n",
    "            #Se realiza una clasificacion, estimando los parametros mediante cv\n",
    "            #svc = get_best_score(X_tr, y_tr)\n",
    "            \n",
    "            svc = SVC(kernel='linear')\n",
    "            svc = svc.fit(X_tr, y_tr)\n",
    "            \n",
    "            b_error = 1-svc.score(X_ts, y_ts)\n",
    "            \n",
    "            #transfer error\n",
    "            #entrenado en dominio src y probado en dominio tgt adaptados\n",
    "            X_tr_a = adapted[src]['X_tr']\n",
    "            y_tr = np.asarray(labeled[src]['y_tr'].todense()).argmax(axis=1)\n",
    "            \n",
    "            X_ts_a = adapted[tgt]['X_ts']\n",
    "            y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "            \n",
    "            svc_a = SVC(kernel='linear')\n",
    "            svc_a = svc_a.fit(X_tr_a, y_tr)\n",
    "            \n",
    "            t_error = 1-svc_a.score(X_ts_a, y_ts)\n",
    "            # transfer loss t\n",
    "            # t_error - b_error\n",
    "            t_loss = t_error - b_error\n",
    "            \n",
    "            tarea = src[0]+'->'+tgt[0]\n",
    "            df.loc[i] = ['SDA','Amazon',tarea,src,tgt,b_error*100,t_error*100, t_loss*100]\n",
    "            i+=1\n",
    "\n",
    "print \"Pruebas completadas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adaptacion</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Tarea</th>\n",
       "      <th>Fuente</th>\n",
       "      <th>Objetivo</th>\n",
       "      <th>Baseline error</th>\n",
       "      <th>Transfer error</th>\n",
       "      <th>Transfer loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>k-&gt;d</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>dvd</td>\n",
       "      <td>28.00</td>\n",
       "      <td>36.75</td>\n",
       "      <td>8.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>k-&gt;e</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>electronics</td>\n",
       "      <td>28.75</td>\n",
       "      <td>36.00</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>k-&gt;b</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>books</td>\n",
       "      <td>32.25</td>\n",
       "      <td>42.00</td>\n",
       "      <td>9.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>d-&gt;k</td>\n",
       "      <td>dvd</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>24.00</td>\n",
       "      <td>39.75</td>\n",
       "      <td>15.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>d-&gt;e</td>\n",
       "      <td>dvd</td>\n",
       "      <td>electronics</td>\n",
       "      <td>28.75</td>\n",
       "      <td>41.00</td>\n",
       "      <td>12.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>d-&gt;b</td>\n",
       "      <td>dvd</td>\n",
       "      <td>books</td>\n",
       "      <td>32.25</td>\n",
       "      <td>36.00</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>e-&gt;k</td>\n",
       "      <td>electronics</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>24.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>e-&gt;d</td>\n",
       "      <td>electronics</td>\n",
       "      <td>dvd</td>\n",
       "      <td>28.00</td>\n",
       "      <td>36.50</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>e-&gt;b</td>\n",
       "      <td>electronics</td>\n",
       "      <td>books</td>\n",
       "      <td>32.25</td>\n",
       "      <td>43.25</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>b-&gt;k</td>\n",
       "      <td>books</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>24.00</td>\n",
       "      <td>37.75</td>\n",
       "      <td>13.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>b-&gt;d</td>\n",
       "      <td>books</td>\n",
       "      <td>dvd</td>\n",
       "      <td>28.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>b-&gt;e</td>\n",
       "      <td>books</td>\n",
       "      <td>electronics</td>\n",
       "      <td>28.75</td>\n",
       "      <td>38.50</td>\n",
       "      <td>9.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Adaptacion Dataset Tarea       Fuente     Objetivo  Baseline error  \\\n",
       "0    Baseline  Amazon  k->d      kitchen          dvd           28.00   \n",
       "1    Baseline  Amazon  k->e      kitchen  electronics           28.75   \n",
       "2    Baseline  Amazon  k->b      kitchen        books           32.25   \n",
       "3    Baseline  Amazon  d->k          dvd      kitchen           24.00   \n",
       "4    Baseline  Amazon  d->e          dvd  electronics           28.75   \n",
       "5    Baseline  Amazon  d->b          dvd        books           32.25   \n",
       "6    Baseline  Amazon  e->k  electronics      kitchen           24.00   \n",
       "7    Baseline  Amazon  e->d  electronics          dvd           28.00   \n",
       "8    Baseline  Amazon  e->b  electronics        books           32.25   \n",
       "9    Baseline  Amazon  b->k        books      kitchen           24.00   \n",
       "10   Baseline  Amazon  b->d        books          dvd           28.00   \n",
       "11   Baseline  Amazon  b->e        books  electronics           28.75   \n",
       "\n",
       "    Transfer error  Transfer loss  \n",
       "0            36.75           8.75  \n",
       "1            36.00           7.25  \n",
       "2            42.00           9.75  \n",
       "3            39.75          15.75  \n",
       "4            41.00          12.25  \n",
       "5            36.00           3.75  \n",
       "6            38.00          14.00  \n",
       "7            36.50           8.50  \n",
       "8            43.25          11.00  \n",
       "9            37.75          13.75  \n",
       "10           31.00           3.00  \n",
       "11           38.50           9.75  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(scores_path+tipo+'/amazon.csv', columns=df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
