{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#adaptacion\n",
    "from utils.adaptacion import adapt_gfk, transform_gfk\n",
    "\n",
    "#clasificadores\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from utils.clasificacion import *\n",
    "\n",
    "#carga de datasets\n",
    "from utils.DatasetStorage import Dataset\n",
    "from utils.paths import *\n",
    "\n",
    "#otros\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#variables para guardar los resultados\n",
    "tipo = pruebas[2]\n",
    "dataset_name = datasets[1]\n",
    "dims = dimensions[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(K, Xs, Ys, Xt, Yt):\n",
    "    np.random.seed(10)\n",
    "\n",
    "    source = np.diag(np.dot(np.dot(Xs, K), Xs.T))\n",
    "    source = np.reshape(source, (Xs.shape[0], 1))\n",
    "    source = np.matlib.repmat(source, 1, Yt.shape[0])\n",
    "    \n",
    "    target = np.diag(np.dot(np.dot(Xt, K), Xt.T))\n",
    "    target = np.reshape(target, (Xt.shape[0], 1))\n",
    "    target = np.matlib.repmat(target, 1, Ys.shape[0]).T\n",
    "    \n",
    "    dist = source + target - 2 * np.dot(np.dot(Xs, K), Xt.T)\n",
    "    \n",
    "    indices = np.argmin(dist, axis=0)\n",
    "    prediction = Ys[indices]\n",
    "    \n",
    "    accuracy = sum(prediction == Yt) / float(Yt.shape[0])\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def compute_knn(K, Xs, Ys, Xt, Yt):\n",
    "    Xs = np.dot(Xs, K)\n",
    "    Xt = np.dot(Xt, K)\n",
    "    \n",
    "    neigh = KNeighborsClassifier().fit(Xs, Ys)\n",
    "    \n",
    "    return neigh.score(Xt, Yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas con el dataset Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfk\n",
      "twitter\n",
      "2000\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "print tipo\n",
    "print dataset_name\n",
    "print dims\n",
    "print data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargando dataset Twitter\n",
    "dataset_path = os.path.join(data_path, dataset_name+'.pkl')\n",
    "dataset_object = Dataset().load(dataset_path)\n",
    "\n",
    "dataset_object.split_dataset(test_size=0.2)\n",
    "\n",
    "labeled = dataset_object.labeled\n",
    "domains = dataset_object.domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea 1 de 2\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Tarea 2 de 2\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Pruebas completadas.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=dataframe_columns)\n",
    "\n",
    "i=0\n",
    "# por cada par posible para adaptar\n",
    "for src in domains:\n",
    "    for tgt in domains:\n",
    "        if src is not tgt:\n",
    "            print \"Tarea %d de 2\" % (i+1)\n",
    "            \n",
    "            \n",
    "            #baseline in-domain error\n",
    "            #e_b(T,T)\n",
    "            #entrenado en dominio tgt y probado en dominio tgt\n",
    "            X_tr = np.asarray(labeled[tgt]['X_tr'][:, :dims].todense())\n",
    "            y_tr = np.asarray(labeled[tgt]['y_tr'].todense()).argmax(axis=1)\n",
    "            \n",
    "            X_ts = np.asarray(labeled[tgt]['X_ts'][:, :dims].todense())\n",
    "            y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "            \n",
    "            # se crean las rutas para cargar o crear los modelos\n",
    "            model_name = \"indomain_%s.pkl\" % (tgt)\n",
    "            model_path = os.path.join(models_path, dataset_name, model_name)\n",
    "            \n",
    "            #Se realiza una clasificacion, estimando los parametros mediante cv\n",
    "            svc = load_best_score(model_path, X_tr, y_tr)\n",
    "            b_error = 1-svc.score(X_ts, y_ts)\n",
    "            \n",
    "            #############\n",
    "            #### GFK ####\n",
    "            #############\n",
    "            \n",
    "            \n",
    "            X_tr = np.asarray(labeled[src]['X_tr'][:, :dims].todense())\n",
    "            y_tr = np.asarray(labeled[src]['y_tr'].todense()).argmax(axis=1)\n",
    "            \n",
    "            X_ts = np.asarray(labeled[tgt]['X_ts'][:, :dims].todense())\n",
    "            y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "            \n",
    "            # se adaptan los dominios usando GFK\n",
    "            print \"Adaptando dominios...\"\n",
    "            gfk = adapt_gfk(X_tr, X_ts, 10, dims/2)\n",
    "            \n",
    "            t_error = compute_accuracy(gfk.G, X_tr, y_tr, X_ts, y_ts)\n",
    "            \n",
    "            # transfer loss t\n",
    "            # t_error - b_error\n",
    "            t_loss = t_error - b_error\n",
    "            \n",
    "            tarea = src[0]+'->'+tgt[0]\n",
    "            df.loc[i] = ['GFK',tarea,src,tgt,b_error*100,t_error*100, t_loss*100]\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "print \"Pruebas completadas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adaptacion</th>\n",
       "      <th>Tarea</th>\n",
       "      <th>Fuente</th>\n",
       "      <th>Objetivo</th>\n",
       "      <th>Baseline error</th>\n",
       "      <th>Transfer error</th>\n",
       "      <th>Transfer loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GFK</td>\n",
       "      <td>r-&gt;t</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>16.081594</td>\n",
       "      <td>58.461538</td>\n",
       "      <td>42.379945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GFK</td>\n",
       "      <td>t-&gt;r</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>20.687361</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>29.312639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Adaptacion Tarea    Fuente  Objetivo  Baseline error  Transfer error  \\\n",
       "0        GFK  r->t   rio2016  thevoice       16.081594       58.461538   \n",
       "1        GFK  t->r  thevoice   rio2016       20.687361       50.000000   \n",
       "\n",
       "   Transfer loss  \n",
       "0      42.379945  \n",
       "1      29.312639  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando en scores/gfk/twitter.csv\n",
      "Resultados guardados.\n"
     ]
    }
   ],
   "source": [
    "new_scores_path = os.path.join(scores_path, tipo, dataset_name+\".csv\")\n",
    "\n",
    "print \"Guardando en %s\" % new_scores_path\n",
    "df.to_csv(new_scores_path, columns=df.columns)\n",
    "print \"Resultados guardados.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arreglar esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=dataframe_columns)\n",
    "\n",
    "i=0\n",
    "# por cada par posible para adaptar\n",
    "for src in domains:\n",
    "    for tgt in domains:\n",
    "        if src is not tgt:\n",
    "            print \"Tarea %d de 2\" % (i+1)\n",
    "            \n",
    "            \n",
    "            #baseline in-domain error\n",
    "            #e_b(T,T)\n",
    "            #entrenado en dominio tgt y probado en dominio tgt\n",
    "            X_tr = np.asarray(labeled[tgt]['X_tr'][:, :dims].todense())\n",
    "            y_tr = np.asarray(labeled[tgt]['y_tr'].todense()).argmax(axis=1)\n",
    "            \n",
    "            X_ts = np.asarray(labeled[tgt]['X_ts'][:, :dims].todense())\n",
    "            y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "            \n",
    "            # se crean las rutas para cargar o crear los modelos\n",
    "            model_name = \"indomain_%s.pkl\" % (tgt)\n",
    "            model_path = os.path.join(models_path, dataset_name, model_name)\n",
    "            \n",
    "            #Se realiza una clasificacion, estimando los parametros mediante cv\n",
    "            svc = load_best_score(model_path, X_tr, y_tr)\n",
    "            b_error = 1-svc.score(X_ts, y_ts)\n",
    "            \n",
    "            #############\n",
    "            #### GFK ####\n",
    "            #############\n",
    "            \n",
    "            \n",
    "            X_tr = np.asarray(labeled[src]['X_tr'][:, :dims].todense())\n",
    "            y_tr = np.asarray(labeled[src]['y_tr'].todense()).argmax(axis=1)\n",
    "            \n",
    "            X_ts = np.asarray(labeled[tgt]['X_ts'][:, :dims].todense())\n",
    "            y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "            \n",
    "            # se adaptan los dominios usando GFK\n",
    "            print \"Adaptando dominios...\"\n",
    "            gfk = adapt_gfk(X_tr, X_ts, 10, dims/2)\n",
    "            \n",
    "            print \"Transformando dominios...\"\n",
    "            X_tr_a = transform_gfk(X_tr, gfk)\n",
    "            X_ts_a = transform_gfk(X_ts, gfk)\n",
    "            \n",
    "            print \"Entrenando clasificador...\"\n",
    "            svc_a = SVC(kernel='linear')\n",
    "            svc_a = svc_a.fit(X_tr_a, y_tr)\n",
    "            \n",
    "            print \"Probando el clasificador\"\n",
    "            t_error = 1-svc_a.score(X_ts_a, y_ts)\n",
    "            \n",
    "            # transfer loss t\n",
    "            # t_error - b_error\n",
    "            t_loss = t_error - b_error\n",
    "            \n",
    "            tarea = src[0]+'->'+tgt[0]\n",
    "            df.loc[i] = ['GFK',tarea,src,tgt,b_error*100,t_error*100, t_loss*100]\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "print \"Pruebas completadas.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
