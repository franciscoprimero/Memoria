{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#adaptacion\n",
    "from utils.adaptacion import gfk_grid_search, transform_gfk\n",
    "\n",
    "#clasificadores\n",
    "from utils.clasificacion import *\n",
    "\n",
    "#carga de datasets\n",
    "from utils.DatasetStorage import Dataset\n",
    "from utils.paths import *\n",
    "\n",
    "#otros\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "import itertools\n",
    "\n",
    "#variables para guardar los resultados\n",
    "tipo = pruebas[2]\n",
    "dataset_name = datasets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptación\n",
    "\n",
    "## Creación de modelos de adaptación.\n",
    "\n",
    "Por cada par de dominios se entrenan distintos modelos según los parámetros establecidos.\n",
    "Al entrenar cada modelo, este es inmediatamente probado, siendo almacenado o descartado según su desempeño.\n",
    "\n",
    "Cada modelo es guardado en la ruta: models/twitter/gfk/me2\\_[dimensiones]\\_[dominio\\_fuente]_[dominio\\_objetivo].pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas con el dataset Twitter (2000 Dimensiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dims = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfk\n",
      "twitter\n",
      "2000\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "print tipo\n",
    "print dataset_name\n",
    "print dims\n",
    "print data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already splitted\n"
     ]
    }
   ],
   "source": [
    "# cargando dataset Twitter\n",
    "dataset_path = os.path.join(data_path, dataset_name+'.pkl')\n",
    "dataset_object = Dataset().load(dataset_path)\n",
    "\n",
    "dataset_object.split_dataset(test_size=0.2)\n",
    "\n",
    "labeled = dataset_object.labeled\n",
    "domains = dataset_object.domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptando entre:\n",
      "rio2016 - thevoice\n",
      "\tn: 10 - d: 500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.788850174216\n",
      "\tn: 20 - d: 500 0.865783972125\n",
      "\tn: 50 - d: 500 0.903275261324\n",
      "\tn: 10 - d: 1000 0.805853658537\n",
      "\tn: 20 - d: 1000 0.853797909408\n",
      "\tn: 50 - d: 1000"
     ]
    }
   ],
   "source": [
    "models_paths = os.path.join(models_path, dataset_name, tipo, 'me2_%d_models_paths.pkl' % dims)\n",
    "\n",
    "parameters = {\n",
    "    'dims': [int(dims/4), int(dims/2)],\n",
    "    'n_subs': [10, 20, 50]\n",
    "}\n",
    "\n",
    "pairs = list(itertools.permutations(domains, 2))\n",
    "\n",
    "#por cada par se entrena un adaptador y se guarda el que mejor resultados entrega\n",
    "print \"Adaptando entre:\"\n",
    "for src, tgt in pairs:\n",
    "    print \"%s - %s\" % (src, tgt)\n",
    "    X_src = labeled[src]['X_tr'][:, :dims].todense()\n",
    "    y_src = np.asarray(labeled[src]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_tgt = labeled[tgt]['X_tr'][:, :dims].todense()\n",
    "\n",
    "\n",
    "    gfk, score = gfk_grid_search(X_src, y_src, X_tgt, parameters, n_jobs=4)\n",
    "    print \"\\tMejor modelo: %.3f\" % score\n",
    "\n",
    "    folder_path = os.path.join(models_path, dataset_name, tipo)\n",
    "    prefix = \"me2_%d_%s_%s.pkl\" % (dims, src, tgt)\n",
    "\n",
    "    best_model_save_path = os.path.join(folder_path, prefix)\n",
    "    print \"\\tGuardando modelo en %s\" % best_model_save_path\n",
    "    joblib.dump(gfk, best_model_save_path)\n",
    "\n",
    "print \"\\nAdaptaciones completadas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pruebas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=dataframe_columns)\n",
    "\n",
    "i=0\n",
    "tareas = len(domains)*(len(domains)-1)\n",
    "pairs = list(itertools.permutations(domains, 2))\n",
    "\n",
    "# por cada par posible para adaptar\n",
    "for src, tgt in pairs:\n",
    "    print \"Tarea %d de %d\" % (i+1, tareas)\n",
    "            \n",
    "    #baseline in-domain error\n",
    "    #e_b(T,T)\n",
    "    #entrenado en dominio tgt y probado en dominio tgt\n",
    "    X_tr = labeled[tgt]['X_tr'][:, :dims].todense()\n",
    "    y_tr = np.asarray(labeled[tgt]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_ts = labeled[tgt]['X_ts'][:, :dims].todense()\n",
    "    y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "\n",
    "    # se crean las rutas para cargar o crear los modelos\n",
    "    model_name = \"%d_%s.pkl\" % (dims, tgt)\n",
    "    model_path = os.path.join(models_path, dataset_name, \"indomain\", model_name)\n",
    "\n",
    "    #Se realiza una clasificacion, estimando los parametros mediante cv\n",
    "    svc = load_best_score(model_path, X_tr, y_tr)\n",
    "    b_error = 1-svc.score(X_ts, y_ts)\n",
    "\n",
    "    #############\n",
    "    #### GFK ####\n",
    "    #############\n",
    "    # se adaptan los dominios usando GFK\n",
    "    print \"Adaptando dominios...\"\n",
    "    folder_path = os.path.join(models_path, dataset_name, tipo)\n",
    "    prefix = \"me2_%d_%s_%s.pkl\" % (dims, src, tgt)\n",
    "\n",
    "    best_model_save_path = os.path.join(folder_path, prefix)\n",
    "\n",
    "    gfk = joblib.load(best_model_save_path)\n",
    "\n",
    "\n",
    "    X_tr = labeled[src]['X_tr'][:, :dims].todense()\n",
    "    X_tr_a = transform_gfk(X_tr, gfk)\n",
    "    y_tr = np.asarray(labeled[src]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_ts = labeled[tgt]['X_ts'][:, :dims].todense()\n",
    "    X_ts_a = transform_gfk(X_ts, gfk)\n",
    "    y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "\n",
    "    clf = get_best_score(X_tr_a, y_tr, classifier='SVC', n_jobs=4)\n",
    "    t_error = 1-clf.score(X_ts_a, y_ts)\n",
    "\n",
    "\n",
    "    # transfer loss t\n",
    "    # t_error - b_error\n",
    "    t_loss = t_error - b_error\n",
    "\n",
    "    tarea = src[0]+'->'+tgt[0]\n",
    "    df.loc[i] = ['GFK',tarea,src,tgt,b_error*100,t_error*100, t_loss*100]\n",
    "\n",
    "    i += 1\n",
    "            \n",
    "print \"\\nPruebas completadas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_scores_path = os.path.join(scores_path,dataset_name, tipo, \"me2_%d.csv\" % (dims))\n",
    "\n",
    "print \"Guardando en %s\" % new_scores_path\n",
    "df.to_csv(new_scores_path, columns=df.columns)\n",
    "print \"Resultados guardados.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas con el dataset Twitter (1000 Dimensiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dims = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print tipo\n",
    "print dataset_name\n",
    "print dims\n",
    "print data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cargando dataset Twitter\n",
    "dataset_path = os.path.join(data_path, dataset_name+'.pkl')\n",
    "dataset_object = Dataset().load(dataset_path)\n",
    "\n",
    "dataset_object.split_dataset(test_size=0.2)\n",
    "\n",
    "labeled = dataset_object.labeled\n",
    "domains = dataset_object.domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_paths = os.path.join(models_path, dataset_name, tipo, 'me2_%d_models_paths.pkl' % dims)\n",
    "\n",
    "parameters = {\n",
    "    'dims': [int(dims/4), int(dims/2)],\n",
    "    'n_subs': [10, 20, 50]\n",
    "}\n",
    "\n",
    "pairs = list(itertools.permutations(domains, 2))\n",
    "\n",
    "#por cada par se entrena un adaptador y se guarda el que mejor resultados entrega\n",
    "print \"Adaptando entre:\"\n",
    "for src, tgt in pairs:\n",
    "    print \"%s - %s\" % (src, tgt)\n",
    "    X_src = labeled[src]['X_tr'][:, :dims].todense()\n",
    "    y_src = np.asarray(labeled[src]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_tgt = labeled[tgt]['X_tr'][:, :dims].todense()\n",
    "\n",
    "\n",
    "    gfk, score = gfk_grid_search(X_src, y_src, X_tgt, parameters, n_jobs=4)\n",
    "    print \"\\tMejor modelo: %.3f\" % score\n",
    "\n",
    "    folder_path = os.path.join(models_path, dataset_name, tipo)\n",
    "    prefix = \"me2_%d_%s_%s.pkl\" % (dims, src, tgt)\n",
    "\n",
    "    best_model_save_path = os.path.join(folder_path, prefix)\n",
    "    print \"\\tGuardando modelo en %s\" % best_model_save_path\n",
    "    joblib.dump(gfk, best_model_save_path)\n",
    "\n",
    "print \"\\nAdaptaciones completadas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pruebas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=dataframe_columns)\n",
    "\n",
    "i=0\n",
    "tareas = len(domains)*(len(domains)-1)\n",
    "pairs = list(itertools.permutations(domains, 2))\n",
    "\n",
    "# por cada par posible para adaptar\n",
    "for src, tgt in pairs:\n",
    "    print \"Tarea %d de %d\" % (i+1, tareas)\n",
    "            \n",
    "    #baseline in-domain error\n",
    "    #e_b(T,T)\n",
    "    #entrenado en dominio tgt y probado en dominio tgt\n",
    "    X_tr = labeled[tgt]['X_tr'][:, :dims].todense()\n",
    "    y_tr = np.asarray(labeled[tgt]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_ts = labeled[tgt]['X_ts'][:, :dims].todense()\n",
    "    y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "\n",
    "    # se crean las rutas para cargar o crear los modelos\n",
    "    model_name = \"%d_%s.pkl\" % (dims, tgt)\n",
    "    model_path = os.path.join(models_path, dataset_name, \"indomain\", model_name)\n",
    "\n",
    "    #Se realiza una clasificacion, estimando los parametros mediante cv\n",
    "    svc = load_best_score(model_path, X_tr, y_tr)\n",
    "    b_error = 1-svc.score(X_ts, y_ts)\n",
    "\n",
    "    #############\n",
    "    #### GFK ####\n",
    "    #############\n",
    "    # se adaptan los dominios usando GFK\n",
    "    print \"Adaptando dominios...\"\n",
    "    folder_path = os.path.join(models_path, dataset_name, tipo)\n",
    "    prefix = \"me2_%d_%s_%s.pkl\" % (dims, src, tgt)\n",
    "\n",
    "    best_model_save_path = os.path.join(folder_path, prefix)\n",
    "\n",
    "    gfk = joblib.load(best_model_save_path)\n",
    "\n",
    "\n",
    "    X_tr = labeled[src]['X_tr'][:, :dims].todense()\n",
    "    X_tr_a = transform_gfk(X_tr, gfk)\n",
    "    y_tr = np.asarray(labeled[src]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_ts = labeled[tgt]['X_ts'][:, :dims].todense()\n",
    "    X_ts_a = transform_gfk(X_ts, gfk)\n",
    "    y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "\n",
    "    clf = get_best_score(X_tr_a, y_tr, classifier='SVC', n_jobs=4)\n",
    "    t_error = 1-clf.score(X_ts_a, y_ts)\n",
    "\n",
    "\n",
    "    # transfer loss t\n",
    "    # t_error - b_error\n",
    "    t_loss = t_error - b_error\n",
    "\n",
    "    tarea = src[0]+'->'+tgt[0]\n",
    "    df.loc[i] = ['GFK',tarea,src,tgt,b_error*100,t_error*100, t_loss*100]\n",
    "\n",
    "    i += 1\n",
    "            \n",
    "print \"\\nPruebas completadas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_scores_path = os.path.join(scores_path,dataset_name, tipo, \"me2_%d.csv\" % (dims))\n",
    "\n",
    "print \"Guardando en %s\" % new_scores_path\n",
    "df.to_csv(new_scores_path, columns=df.columns)\n",
    "print \"Resultados guardados.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
