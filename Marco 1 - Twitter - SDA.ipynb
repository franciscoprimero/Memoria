{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#carga de datasets\n",
    "from utils.DatasetStorage import Dataset\n",
    "from utils.paths import *\n",
    "\n",
    "#clasificadores\n",
    "from utils.clasificacion import *\n",
    "\n",
    "#adaptacion\n",
    "from utils.adaptacion import sda_pseudo_grid_search\n",
    "from keras.models import load_model\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "#variables para guardar los resultados\n",
    "tipo = pruebas[4]\n",
    "dataset_name = datasets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptación\n",
    "\n",
    "\n",
    "## Creación de modelos de adaptación.\n",
    "\n",
    "Para cada dominio se entrenan distintos modelos según los parámetros enviados.\n",
    "\n",
    "Cada modelo es guardado en la ruta: models/twitter/sda/me1\\_[dominio\\_objetivo]_[numero_de_modelo].pkl\n",
    "\n",
    "Todas las rutas son guardadas en una lista, la cual es almacenada en: models/twitter/sda/me1_models_paths.pkl\n",
    "\n",
    "```python\n",
    "paths_list = [{'autoencoder': ruta_autoencoder, 'encoder': ruta_encoder}, ..., {'autoencoder': ruta_autoencoder, 'encoder': ruta_encoder}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda del mejor modelo por dominio\n",
    "\n",
    "Por cada dominio se busca el mejor modelo de adaptación.\n",
    "\n",
    "Esto se obtiene adaptando los datos de entrenamiento de cada dominio por cada modelo creado y realizando Grid-Search y Cross-Validation con estos datos.\n",
    "\n",
    "El modelo que logre un mejor valor de Cross-Validation es considerado el mejor modelo para adaptar y queda guardado en un diccionario de la forma:\n",
    "\n",
    "```python\n",
    "best_models = {\n",
    "    [dominio_1] = {'autoencoder': ruta_autoencoder, 'encoder': ruta_encoder},\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    [dominio_n] = {'autoencoder': ruta_autoencoder, 'encoder': ruta_encoder},\n",
    "}\n",
    "```\n",
    "\n",
    "Este diccionario queda almacenado en la ruta:\n",
    "    models/twitter/sda/me1_best_models.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pruebas con el dataset de Twitter (2000 Dimensiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dims = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sda\n",
      "twitter\n",
      "data\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print tipo\n",
    "print dataset_name\n",
    "print data_path\n",
    "print dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already splitted\n"
     ]
    }
   ],
   "source": [
    "# cargando dataset de Twitter\n",
    "dataset_path = os.path.join(data_path, dataset_name+'.pkl')\n",
    "dataset_object = Dataset().load(dataset_path)\n",
    "\n",
    "dataset_object.split_dataset(test_size=0.2)\n",
    "\n",
    "labeled = dataset_object.labeled\n",
    "domains = dataset_object.domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4635, 2000)\n",
      "Todos los datos disponibles obtenidos\n"
     ]
    }
   ],
   "source": [
    "# se obtienen todos los valores X disponibles para realizar adaptacion\n",
    "X = dataset_object.get_all_X()\n",
    "X = np.asarray(X[: , :dims])\n",
    "\n",
    "print X.shape\n",
    "\n",
    "print \"Todos los datos disponibles obtenidos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3708, 2000)\n",
      "(927, 2000)\n"
     ]
    }
   ],
   "source": [
    "# se divide el dataset para los datos de entrenamiento y validacion del SDA\n",
    "X_train, X_val, _, _ = train_test_split(X, np.zeros(X.shape[0]), test_size=0.2, random_state=42)\n",
    "\n",
    "print X_train.shape\n",
    "print X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando modelos de adaptacion...\n",
      "pr: 0.300 - epochs: 50 - layers: [1000]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_0.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_0.h5\n",
      "pr: 0.300 - epochs: 25 - layers: [1000]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_1.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_1.h5\n",
      "pr: 0.300 - epochs: 50 - layers: [1000, 500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_2.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_2.h5\n",
      "pr: 0.300 - epochs: 25 - layers: [1000, 500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_3.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_3.h5\n",
      "pr: 0.500 - epochs: 50 - layers: [1000]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_4.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_4.h5\n",
      "pr: 0.500 - epochs: 25 - layers: [1000]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_5.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_5.h5\n",
      "pr: 0.500 - epochs: 50 - layers: [1000, 500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_6.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_6.h5\n",
      "pr: 0.500 - epochs: 25 - layers: [1000, 500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_7.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_7.h5\n",
      "pr: 0.800 - epochs: 50 - layers: [1000]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_8.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_8.h5\n",
      "pr: 0.800 - epochs: 25 - layers: [1000]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_9.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_9.h5\n",
      "pr: 0.800 - epochs: 50 - layers: [1000, 500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_10.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_10.h5\n",
      "pr: 0.800 - epochs: 25 - layers: [1000, 500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_2000_ae_11.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_2000_e_11.h5\n",
      "\n",
      "Creacion de modelos terminada\n",
      "Guardando rutas en models/twitter/sda/me1_2000_models_paths.pkl\n",
      "Rutas cargadas en la variable 'paths_list'\n"
     ]
    }
   ],
   "source": [
    "models_paths = os.path.join(models_path, dataset_name, tipo, 'me1_%d_models_paths.pkl' % dims)\n",
    "paths_list = []\n",
    "\n",
    "# si existe el archivo con las rutas\n",
    "# se carga la lista con las rutas\n",
    "if os.path.exists(models_paths):\n",
    "    print \"Cargando rutas de modelos adaptados.\"\n",
    "    paths_list = joblib.load(models_paths)\n",
    "    print \"Rutas cargadas en la variable 'paths_list'\"\n",
    "# si no\n",
    "# se entrenan los modelos y se obtiene la lista con rutas\n",
    "else:\n",
    "    #se establecen los parametros para los modelos\n",
    "    parametros = {\n",
    "        'noises': [0.3 , 0.5, 0.8],\n",
    "        'layers': [[int(dims/2)], [int(dims/2), int(dims/4)]],\n",
    "        'epochs': [50, 25],\n",
    "    }\n",
    "\n",
    "    print \"Creando modelos de adaptacion...\"\n",
    "    \n",
    "    folder_path = os.path.join(models_path, dataset_name, tipo)\n",
    "    prefix = \"me1_%d_\" % dims\n",
    "        \n",
    "    paths_list = sda_pseudo_grid_search(X_train, X_val, parametros, folder_path, prefix)\n",
    "\n",
    "    print \"\\nCreacion de modelos terminada\\nGuardando rutas en %s\" % models_paths\n",
    "    joblib.dump(paths_list, models_paths)\n",
    "    print \"Rutas cargadas en la variable 'paths_list'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos almacenados en:\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_0.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_0.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_1.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_1.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_2.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_2.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_3.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_3.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_4.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_4.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_5.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_5.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_6.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_6.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_7.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_7.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_8.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_8.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_9.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_9.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_10.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_10.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_2000_ae_11.h5\n",
      "encoder - models/twitter/sda/me1_2000_e_11.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Modelos almacenados en:\"\n",
    "for rutas in paths_list:\n",
    "    print \"\\tautoencoder- %s\\nencoder - %s\\n\" % (rutas['autoencoder'], rutas['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo mejores modelos...\n",
      "rio2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/keras/models.py:240: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1 - score: 0.8856\n",
      "\t2 - score: 0.8884\n",
      "\t3 - score: 0.8454\n",
      "\t4 - score: 0.8394\n",
      "\t5 - score: 0.8864\n",
      "\t6 - score: 0.8976\n",
      "\t7 - score: 0.8665\n",
      "\t8 - score: 0.8569\n",
      "\t9 - score: 0.9157\n",
      "\t10 - score: 0.8917\n",
      "\t11 - score: 0.8669\n",
      "\t12 - score: 0.8637\n",
      "thevoice\n",
      "\t1 - score: 0.8894\n",
      "\t2 - score: 0.8959\n",
      "\t3 - score: 0.8550\n",
      "\t4 - score: 0.8647\n",
      "\t5 - score: 0.8802\n",
      "\t6 - score: 0.8864\n",
      "\t7 - score: 0.8598\n",
      "\t8 - score: 0.8492\n",
      "\t9 - score: 0.8623\n",
      "\t10 - score: 0.8813\n",
      "\t11 - score: 0.8483\n",
      "\t12 - score: 0.8409\n",
      "general\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1 - score: 0.7203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t2 - score: 0.7214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t3 - score: 0.7095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t4 - score: 0.7110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t5 - score: 0.7261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t6 - score: 0.7216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t7 - score: 0.7108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t8 - score: 0.7013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t9 - score: 0.7276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t10 - score: 0.7131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t11 - score: 0.7011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t12 - score: 0.7034\n",
      "Rutas guardadas en  models/twitter/sda/me1_2000_best_models.pkl\n"
     ]
    }
   ],
   "source": [
    "best_models_paths = os.path.join(models_path, dataset_name, tipo, \"me1_%d_best_models.pkl\" % dims)\n",
    "best_models = {}\n",
    "\n",
    "if os.path.exists(best_models_paths):\n",
    "    print \"Cargando rutas de los mejores modelos...\"\n",
    "    best_models = joblib.load(best_models_paths)\n",
    "    print \"Rutas cargadas\"  \n",
    "else:\n",
    "    print \"Obteniendo mejores modelos...\"\n",
    "    for domain in domains:\n",
    "        print domain\n",
    "        i = 1\n",
    "        best_score = 0\n",
    "        best_path = None\n",
    "        best_model = None\n",
    "        \n",
    "        for sda_model_path in paths_list:\n",
    "            encoder = load_model(sda_model_path['encoder'])\n",
    "\n",
    "            # se obtienen los datos del dominio\n",
    "            X_tr = labeled[domain]['X_tr'][:, :dims].todense()\n",
    "            y_tr = np.asarray(labeled[domain]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "            #se adapta el dominio segun el modelo\n",
    "            X_deep = encoder.predict(X_tr)\n",
    "\n",
    "            # se obtiene el mejor score con GridSearch\n",
    "            new_clf = get_best_score(X_deep, y_tr, classifier='SVC', n_jobs=4)\n",
    "            new_score = new_clf.best_score_\n",
    "            \n",
    "            print \"\\t%d - score: %.4f\" % (i, new_score)\n",
    "            \n",
    "            # se guarda si es el mejor para el modelo\n",
    "            if new_score > best_score:\n",
    "                best_score = new_score\n",
    "                best_path = sda_model_path['encoder']\n",
    "            i = i+1\n",
    "\n",
    "        #se guarda el mejor modelo para este dominio\n",
    "        best_models[domain] = best_path\n",
    "\n",
    "    # se guarda el diccionario con las mejores rutas\n",
    "    joblib.dump(best_models, best_models_paths)\n",
    "    print \"Rutas guardadas en \", best_models_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thevoice - models/twitter/sda/me1_2000_e_1.h5\n",
      "rio2016 - models/twitter/sda/me1_2000_e_8.h5\n",
      "general - models/twitter/sda/me1_2000_e_8.h5\n"
     ]
    }
   ],
   "source": [
    "for dominio, ruta in best_models.items():\n",
    "    print \"%s - %s\" % (dominio, ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando mejor modelo para rio2016\n",
      "Adaptando dominio\n",
      "Cargando mejor modelo para thevoice\n",
      "Adaptando dominio\n",
      "Cargando mejor modelo para general\n",
      "Adaptando dominio\n",
      "3/3 dominios adaptados\n"
     ]
    }
   ],
   "source": [
    "#diccionario para mantener los dominios adaptados\n",
    "adapted = {}\n",
    "\n",
    "i = 0\n",
    "for domain in domains:\n",
    "    best_model_path = best_models[domain]\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        print \"Cargando mejor modelo para %s\" % domain\n",
    "        encoder = load_model(best_model_path)\n",
    "        \n",
    "        print \"Adaptando dominio\"\n",
    "        X_tr = labeled[domain]['X_tr'][:, :dims].todense()\n",
    "        \n",
    "        tr_reps = encoder.predict(X_tr)\n",
    "\n",
    "        adapted[domain] = {\n",
    "            'X_tr': tr_reps,\n",
    "        }\n",
    "        \n",
    "        i = i+1\n",
    "    else:\n",
    "        print \"Generar mejor modelo para %s\" % domain\n",
    "        \n",
    "print \"%d/%d dominios adaptados\" % (i, len(domains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea 1 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "Tarea 2 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "Tarea 3 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "Tarea 4 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "Tarea 5 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea 6 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruebas completadas.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=dataframe_columns)\n",
    "\n",
    "i=0\n",
    "tareas = len(domains)*(len(domains)-1)\n",
    "pairs = list(itertools.permutations(domains, 2))\n",
    "\n",
    "# por cada par posible para adaptar\n",
    "for src, tgt in pairs:\n",
    "    #se carga el mejor modelo para el dominio fuente\n",
    "    ruta = best_models[src]\n",
    "    sda_adapter = load_model(ruta)\n",
    "\n",
    "    print \"Tarea %d de %d\" % (i+1, tareas)\n",
    "\n",
    "    #baseline in-domain error\n",
    "    #e_b(T,T)\n",
    "    #entrenado en dominio tgt y probado en dominio tgt\n",
    "    X_tr = labeled[tgt]['X_tr'][:, :dims].todense()\n",
    "    y_tr = np.asarray(labeled[tgt]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_ts = labeled[tgt]['X_ts'][:, :dims].todense()\n",
    "    y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "\n",
    "    # se crean las rutas para cargar o crear los modelos\n",
    "    model_name = \"%d_%s.pkl\" % (dims, tgt)\n",
    "    model_path = os.path.join(models_path, dataset_name, \"indomain\", model_name)\n",
    "\n",
    "    #Se realiza una clasificacion, estimando los parametros mediante cv\n",
    "    svc = load_best_score(model_path, X_tr, y_tr)\n",
    "    b_error = 1-svc.score(X_ts, y_ts)\n",
    "\n",
    "\n",
    "    #############\n",
    "    #### SDA ####\n",
    "    #############\n",
    "    print \"Adaptando dominios...\"\n",
    "\n",
    "    #entrenado en dominio src y probado en dominio tgt adaptados\n",
    "    X_tr_a = adapted[src]['X_tr']\n",
    "    y_tr = np.asarray(labeled[src]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_ts_a = sda_adapter.predict(X_ts)\n",
    "    y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "\n",
    "    print \"Entrenando clasificador adaptado.\"\n",
    "    svc_a = get_best_score(X_tr_a, y_tr, classifier='SVC', n_jobs=4)\n",
    "    t_error = 1-svc_a.score(X_ts_a, y_ts)\n",
    "\n",
    "    # transfer loss t\n",
    "    # t_error - b_error\n",
    "    t_loss = t_error - b_error\n",
    "\n",
    "    tarea = src[0]+'->'+tgt[0]\n",
    "    df.loc[i] = ['SDA',tarea,src,tgt,b_error*100,t_error*100, t_loss*100]\n",
    "    i+=1\n",
    "\n",
    "print \"\\nPruebas completadas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adaptacion</th>\n",
       "      <th>Tarea</th>\n",
       "      <th>Fuente</th>\n",
       "      <th>Objetivo</th>\n",
       "      <th>Baseline error</th>\n",
       "      <th>Transfer error</th>\n",
       "      <th>Transfer loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SDA</td>\n",
       "      <td>r-&gt;t</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>7.362689</td>\n",
       "      <td>20.075758</td>\n",
       "      <td>12.713068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDA</td>\n",
       "      <td>r-&gt;g</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>general</td>\n",
       "      <td>26.998127</td>\n",
       "      <td>42.417628</td>\n",
       "      <td>15.419501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SDA</td>\n",
       "      <td>t-&gt;r</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>10.208711</td>\n",
       "      <td>13.702359</td>\n",
       "      <td>3.493648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SDA</td>\n",
       "      <td>t-&gt;g</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>general</td>\n",
       "      <td>26.998127</td>\n",
       "      <td>39.635490</td>\n",
       "      <td>12.637363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SDA</td>\n",
       "      <td>g-&gt;r</td>\n",
       "      <td>general</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>10.208711</td>\n",
       "      <td>26.769510</td>\n",
       "      <td>16.560799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SDA</td>\n",
       "      <td>g-&gt;t</td>\n",
       "      <td>general</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>7.362689</td>\n",
       "      <td>28.290720</td>\n",
       "      <td>20.928030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Adaptacion Tarea    Fuente  Objetivo  Baseline error  Transfer error  \\\n",
       "0        SDA  r->t   rio2016  thevoice        7.362689       20.075758   \n",
       "1        SDA  r->g   rio2016   general       26.998127       42.417628   \n",
       "2        SDA  t->r  thevoice   rio2016       10.208711       13.702359   \n",
       "3        SDA  t->g  thevoice   general       26.998127       39.635490   \n",
       "4        SDA  g->r   general   rio2016       10.208711       26.769510   \n",
       "5        SDA  g->t   general  thevoice        7.362689       28.290720   \n",
       "\n",
       "   Transfer loss  \n",
       "0      12.713068  \n",
       "1      15.419501  \n",
       "2       3.493648  \n",
       "3      12.637363  \n",
       "4      16.560799  \n",
       "5      20.928030  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando en scores/twitter/sda/me1_2000.csv\n",
      "Resultados guardados.\n"
     ]
    }
   ],
   "source": [
    "new_scores_path = os.path.join(scores_path,dataset_name, tipo, \"me1_%d.csv\" % (dims))\n",
    "\n",
    "print \"Guardando en %s\" % new_scores_path\n",
    "df.to_csv(new_scores_path, columns=df.columns)\n",
    "print \"Resultados guardados.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pruebas con el dataset de Twitter (1000 Dimensiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dims = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sda\n",
      "twitter\n",
      "data\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print tipo\n",
    "print dataset_name\n",
    "print data_path\n",
    "print dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already splitted\n"
     ]
    }
   ],
   "source": [
    "# cargando dataset de Twitter\n",
    "dataset_path = os.path.join(data_path, dataset_name+'.pkl')\n",
    "dataset_object = Dataset().load(dataset_path)\n",
    "\n",
    "dataset_object.split_dataset(test_size=0.2)\n",
    "\n",
    "labeled = dataset_object.labeled\n",
    "domains = dataset_object.domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4635, 1000)\n",
      "Todos los datos disponibles obtenidos\n"
     ]
    }
   ],
   "source": [
    "# se obtienen todos los valores X disponibles para realizar adaptacion\n",
    "X = dataset_object.get_all_X()\n",
    "X = np.asarray(X[: , :dims])\n",
    "\n",
    "print X.shape\n",
    "\n",
    "print \"Todos los datos disponibles obtenidos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3708, 1000)\n",
      "(927, 1000)\n"
     ]
    }
   ],
   "source": [
    "# se divide el dataset para los datos de entrenamiento y validacion del SDA\n",
    "X_train, X_val, _, _ = train_test_split(X, np.zeros(X.shape[0]), test_size=0.2, random_state=42)\n",
    "\n",
    "print X_train.shape\n",
    "print X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando modelos de adaptacion...\n",
      "pr: 0.300 - epochs: 50 - layers: [500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_0.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_0.h5\n",
      "pr: 0.300 - epochs: 25 - layers: [500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_1.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_1.h5\n",
      "pr: 0.300 - epochs: 50 - layers: [500, 250]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_2.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_2.h5\n",
      "pr: 0.300 - epochs: 25 - layers: [500, 250]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_3.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_3.h5\n",
      "pr: 0.500 - epochs: 50 - layers: [500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_4.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_4.h5\n",
      "pr: 0.500 - epochs: 25 - layers: [500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_5.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_5.h5\n",
      "pr: 0.500 - epochs: 50 - layers: [500, 250]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_6.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_6.h5\n",
      "pr: 0.500 - epochs: 25 - layers: [500, 250]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_7.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_7.h5\n",
      "pr: 0.800 - epochs: 50 - layers: [500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_8.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_8.h5\n",
      "pr: 0.800 - epochs: 25 - layers: [500]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_9.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_9.h5\n",
      "pr: 0.800 - epochs: 50 - layers: [500, 250]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_10.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_10.h5\n",
      "pr: 0.800 - epochs: 25 - layers: [500, 250]\n",
      "\tEntrenando autoencoder...\n",
      "\tGuardando autoencoder en models/twitter/sda/me1_1000_ae_11.h5\n",
      "\tGuardando encoder en models/twitter/sda/me1_1000_e_11.h5\n",
      "\n",
      "Creacion de modelos terminada\n",
      "Guardando rutas en models/twitter/sda/me1_1000_models_paths.pkl\n",
      "Rutas cargadas en la variable 'paths_list'\n"
     ]
    }
   ],
   "source": [
    "models_paths = os.path.join(models_path, dataset_name, tipo, 'me1_%d_models_paths.pkl' % dims)\n",
    "paths_list = []\n",
    "\n",
    "# si existe el archivo con las rutas\n",
    "# se carga la lista con las rutas\n",
    "if os.path.exists(models_paths):\n",
    "    print \"Cargando rutas de modelos adaptados.\"\n",
    "    paths_list = joblib.load(models_paths)\n",
    "    print \"Rutas cargadas en la variable 'paths_list'\"\n",
    "# si no\n",
    "# se entrenan los modelos y se obtiene la lista con rutas\n",
    "else:\n",
    "    #se establecen los parametros para los modelos\n",
    "    parametros = {\n",
    "        'noises': [0.3 , 0.5, 0.8],\n",
    "        'layers': [[int(dims/2)], [int(dims/2), int(dims/4)]],\n",
    "        'epochs': [50, 25],\n",
    "    }\n",
    "\n",
    "    print \"Creando modelos de adaptacion...\"\n",
    "    \n",
    "    folder_path = os.path.join(models_path, dataset_name, tipo)\n",
    "    prefix = \"me1_%d_\" % dims\n",
    "        \n",
    "    paths_list = sda_pseudo_grid_search(X_train, X_val, parametros, folder_path, prefix)\n",
    "\n",
    "    print \"\\nCreacion de modelos terminada\\nGuardando rutas en %s\" % models_paths\n",
    "    joblib.dump(paths_list, models_paths)\n",
    "    print \"Rutas cargadas en la variable 'paths_list'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos almacenados en:\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_0.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_0.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_1.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_1.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_2.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_2.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_3.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_3.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_4.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_4.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_5.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_5.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_6.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_6.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_7.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_7.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_8.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_8.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_9.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_9.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_10.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_10.h5\n",
      "\n",
      "\tautoencoder- models/twitter/sda/me1_1000_ae_11.h5\n",
      "encoder - models/twitter/sda/me1_1000_e_11.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Modelos almacenados en:\"\n",
    "for rutas in paths_list:\n",
    "    print \"\\tautoencoder- %s\\nencoder - %s\\n\" % (rutas['autoencoder'], rutas['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo mejores modelos...\n",
      "rio2016\n",
      "\t1 - score: 0.8654\n",
      "\t2 - score: 0.8583\n",
      "\t3 - score: 0.8341\n",
      "\t4 - score: 0.8244\n",
      "\t5 - score: 0.8733\n",
      "\t6 - score: 0.8468\n",
      "\t7 - score: 0.8222\n",
      "\t8 - score: 0.7762\n",
      "\t9 - score: 0.8853\n",
      "\t10 - score: 0.8626\n",
      "\t11 - score: 0.8256\n",
      "\t12 - score: 0.8431\n",
      "thevoice\n",
      "\t1 - score: 0.8265\n",
      "\t2 - score: 0.8098\n",
      "\t3 - score: 0.8147\n",
      "\t4 - score: 0.7721\n",
      "\t5 - score: 0.8400\n",
      "\t6 - score: 0.8100\n",
      "\t7 - score: 0.7929\n",
      "\t8 - score: 0.7818\n",
      "\t9 - score: 0.8354\n",
      "\t10 - score: 0.8304\n",
      "\t11 - score: 0.7578\n",
      "\t12 - score: 0.7831\n",
      "general\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1 - score: 0.6564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t2 - score: 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t3 - score: 0.6242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t4 - score: 0.6444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ubuntu/anaconda2/envs/env_memoria/lib/python2.7/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t5 - score: 0.6568\n",
      "\t6 - score: 0.6318\n",
      "\t7 - score: 0.6233\n",
      "\t8 - score: 0.6410\n",
      "\t9 - score: 0.6522\n",
      "\t10 - score: 0.6516\n",
      "\t11 - score: 0.6176\n",
      "\t12 - score: 0.6287\n",
      "Rutas guardadas en  models/twitter/sda/me1_1000_best_models.pkl\n"
     ]
    }
   ],
   "source": [
    "best_models_paths = os.path.join(models_path, dataset_name, tipo, \"me1_%d_best_models.pkl\" % dims)\n",
    "best_models = {}\n",
    "\n",
    "if os.path.exists(best_models_paths):\n",
    "    print \"Cargando rutas de los mejores modelos...\"\n",
    "    best_models = joblib.load(best_models_paths)\n",
    "    print \"Rutas cargadas\"  \n",
    "else:\n",
    "    print \"Obteniendo mejores modelos...\"\n",
    "    for domain in domains:\n",
    "        print domain\n",
    "        i = 1\n",
    "        best_score = 0\n",
    "        best_path = None\n",
    "        best_model = None\n",
    "        \n",
    "        for sda_model_path in paths_list:\n",
    "            encoder = load_model(sda_model_path['encoder'])\n",
    "\n",
    "            # se obtienen los datos del dominio\n",
    "            X_tr = labeled[domain]['X_tr'][:, :dims].todense()\n",
    "            y_tr = np.asarray(labeled[domain]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "            #se adapta el dominio segun el modelo\n",
    "            X_deep = encoder.predict(X_tr)\n",
    "\n",
    "            # se obtiene el mejor score con GridSearch\n",
    "            new_clf = get_best_score(X_deep, y_tr, classifier='SVC', n_jobs=4)\n",
    "            new_score = new_clf.best_score_\n",
    "            \n",
    "            print \"\\t%d - score: %.4f\" % (i, new_score)\n",
    "            \n",
    "            # se guarda si es el mejor para el modelo\n",
    "            if new_score > best_score:\n",
    "                best_score = new_score\n",
    "                best_path = sda_model_path['encoder']\n",
    "            i = i+1\n",
    "\n",
    "        #se guarda el mejor modelo para este dominio\n",
    "        best_models[domain] = best_path\n",
    "\n",
    "    # se guarda el diccionario con las mejores rutas\n",
    "    joblib.dump(best_models, best_models_paths)\n",
    "    print \"Rutas guardadas en \", best_models_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thevoice - models/twitter/sda/me1_1000_e_4.h5\n",
      "rio2016 - models/twitter/sda/me1_1000_e_8.h5\n",
      "general - models/twitter/sda/me1_1000_e_4.h5\n"
     ]
    }
   ],
   "source": [
    "for dominio, ruta in best_models.items():\n",
    "    print \"%s - %s\" % (dominio, ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando mejor modelo para rio2016\n",
      "Adaptando dominio\n",
      "Cargando mejor modelo para thevoice\n",
      "Adaptando dominio\n",
      "Cargando mejor modelo para general\n",
      "Adaptando dominio\n",
      "3/3 dominios adaptados\n"
     ]
    }
   ],
   "source": [
    "#diccionario para mantener los dominios adaptados\n",
    "adapted = {}\n",
    "\n",
    "i = 0\n",
    "for domain in domains:\n",
    "    best_model_path = best_models[domain]\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        print \"Cargando mejor modelo para %s\" % domain\n",
    "        encoder = load_model(best_model_path)\n",
    "        \n",
    "        print \"Adaptando dominio\"\n",
    "        X_tr = labeled[domain]['X_tr'][:, :dims].todense()\n",
    "        \n",
    "        tr_reps = encoder.predict(X_tr)\n",
    "\n",
    "        adapted[domain] = {\n",
    "            'X_tr': tr_reps,\n",
    "        }\n",
    "        \n",
    "        i = i+1\n",
    "    else:\n",
    "        print \"Generar mejor modelo para %s\" % domain\n",
    "        \n",
    "print \"%d/%d dominios adaptados\" % (i, len(domains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea 1 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "Tarea 2 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "Tarea 3 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "Tarea 4 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "Tarea 5 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "Tarea 6 de 6\n",
      "Cargando modelo existente.\n",
      "Adaptando dominios...\n",
      "Entrenando clasificador adaptado.\n",
      "\n",
      "Pruebas completadas.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=dataframe_columns)\n",
    "\n",
    "i=0\n",
    "tareas = len(domains)*(len(domains)-1)\n",
    "pairs = list(itertools.permutations(domains, 2))\n",
    "\n",
    "# por cada par posible para adaptar\n",
    "for src, tgt in pairs:\n",
    "    #se carga el mejor modelo para el dominio fuente\n",
    "    ruta = best_models[src]\n",
    "    sda_adapter = load_model(ruta)\n",
    "\n",
    "    print \"Tarea %d de %d\" % (i+1, tareas)\n",
    "\n",
    "    #baseline in-domain error\n",
    "    #e_b(T,T)\n",
    "    #entrenado en dominio tgt y probado en dominio tgt\n",
    "    X_tr = labeled[tgt]['X_tr'][:, :dims].todense()\n",
    "    y_tr = np.asarray(labeled[tgt]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_ts = labeled[tgt]['X_ts'][:, :dims].todense()\n",
    "    y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "\n",
    "    # se crean las rutas para cargar o crear los modelos\n",
    "    model_name = \"%d_%s.pkl\" % (dims, tgt)\n",
    "    model_path = os.path.join(models_path, dataset_name, \"indomain\", model_name)\n",
    "\n",
    "    #Se realiza una clasificacion, estimando los parametros mediante cv\n",
    "    svc = load_best_score(model_path, X_tr, y_tr)\n",
    "    b_error = 1-svc.score(X_ts, y_ts)\n",
    "\n",
    "\n",
    "    #############\n",
    "    #### SDA ####\n",
    "    #############\n",
    "    print \"Adaptando dominios...\"\n",
    "\n",
    "    #entrenado en dominio src y probado en dominio tgt adaptados\n",
    "    X_tr_a = adapted[src]['X_tr']\n",
    "    y_tr = np.asarray(labeled[src]['y_tr'].todense()).argmax(axis=1)\n",
    "\n",
    "    X_ts_a = sda_adapter.predict(X_ts)\n",
    "    y_ts = np.asarray(labeled[tgt]['y_ts'].todense()).argmax(axis=1)\n",
    "\n",
    "    print \"Entrenando clasificador adaptado.\"\n",
    "    svc_a = get_best_score(X_tr_a, y_tr, classifier='SVC', n_jobs=4)\n",
    "    t_error = 1-svc_a.score(X_ts_a, y_ts)\n",
    "\n",
    "    # transfer loss t\n",
    "    # t_error - b_error\n",
    "    t_loss = t_error - b_error\n",
    "\n",
    "    tarea = src[0]+'->'+tgt[0]\n",
    "    df.loc[i] = ['SDA',tarea,src,tgt,b_error*100,t_error*100, t_loss*100]\n",
    "    i+=1\n",
    "\n",
    "print \"\\nPruebas completadas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adaptacion</th>\n",
       "      <th>Tarea</th>\n",
       "      <th>Fuente</th>\n",
       "      <th>Objetivo</th>\n",
       "      <th>Baseline error</th>\n",
       "      <th>Transfer error</th>\n",
       "      <th>Transfer loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SDA</td>\n",
       "      <td>r-&gt;t</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>19.045928</td>\n",
       "      <td>28.527462</td>\n",
       "      <td>9.481534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDA</td>\n",
       "      <td>r-&gt;g</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>general</td>\n",
       "      <td>30.407980</td>\n",
       "      <td>45.876367</td>\n",
       "      <td>15.468387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SDA</td>\n",
       "      <td>t-&gt;r</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>7.372958</td>\n",
       "      <td>21.302178</td>\n",
       "      <td>13.929220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SDA</td>\n",
       "      <td>t-&gt;g</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>general</td>\n",
       "      <td>30.407980</td>\n",
       "      <td>47.377372</td>\n",
       "      <td>16.969392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SDA</td>\n",
       "      <td>g-&gt;r</td>\n",
       "      <td>general</td>\n",
       "      <td>rio2016</td>\n",
       "      <td>7.372958</td>\n",
       "      <td>32.055354</td>\n",
       "      <td>24.682396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SDA</td>\n",
       "      <td>g-&gt;t</td>\n",
       "      <td>general</td>\n",
       "      <td>thevoice</td>\n",
       "      <td>19.045928</td>\n",
       "      <td>27.343750</td>\n",
       "      <td>8.297822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Adaptacion Tarea    Fuente  Objetivo  Baseline error  Transfer error  \\\n",
       "0        SDA  r->t   rio2016  thevoice       19.045928       28.527462   \n",
       "1        SDA  r->g   rio2016   general       30.407980       45.876367   \n",
       "2        SDA  t->r  thevoice   rio2016        7.372958       21.302178   \n",
       "3        SDA  t->g  thevoice   general       30.407980       47.377372   \n",
       "4        SDA  g->r   general   rio2016        7.372958       32.055354   \n",
       "5        SDA  g->t   general  thevoice       19.045928       27.343750   \n",
       "\n",
       "   Transfer loss  \n",
       "0       9.481534  \n",
       "1      15.468387  \n",
       "2      13.929220  \n",
       "3      16.969392  \n",
       "4      24.682396  \n",
       "5       8.297822  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando en scores/twitter/sda/me1_1000.csv\n",
      "Resultados guardados.\n"
     ]
    }
   ],
   "source": [
    "new_scores_path = os.path.join(scores_path,dataset_name, tipo, \"me1_%d.csv\" % (dims))\n",
    "\n",
    "print \"Guardando en %s\" % new_scores_path\n",
    "df.to_csv(new_scores_path, columns=df.columns)\n",
    "print \"Resultados guardados.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
